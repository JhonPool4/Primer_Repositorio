{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1 (parte 1): Regresión Lineal\n",
    "\n",
    "## Sugerencias\n",
    "\n",
    "Se sugiere tener en cuenta los siguientes puntos para el uso de Python en esta tarea.\n",
    "\n",
    "- Los índices en Python comienzan con cero (al igual que en C++). \n",
    "\n",
    "- En la tarea se debe usar arreglos `numpy` (y no \"listas\" ni \"tuplas\") debido a que las operaciones con vectores y matrices funcionan solamente con `numpy`.\n",
    "\n",
    "- Si aparecen errores al ejecutar el código, se sugiere inspeccionar las operaciones matriciales para asegurarse que las dimensiones son compatibles. Se puede obtener las dimensiones de los arreglos utilizando el método `shape`.\n",
    "\n",
    "- Tener cuidado con la multiplicación en `numpy`. Si se tiene dos matrices (arreglos de `numpy`) `A` y `B`, la operación `A*B` realiza el producto término a término, y no el producto matricial. Para obtener el producto matricial se debe utilizar la función `dot` de la siguiente manera: `A.dot(B)` o `np.dot(A,B)` (a partir de Python 3.5 también se puede utilizar `A@B` para realizar el producto matricial).\n",
    "\n",
    "- Se debe ejecutar con `Ctrl+enter` todas las celdas de código (desde el inicio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se cargará las bibliotecas necesarias para esta tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba que si guarda xd\n"
     ]
    }
   ],
   "source": [
    "# Biblioteca para el uso de matrices y vectores\n",
    "import numpy as np\n",
    "# Biblioteca para gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "# Módulos para graficar superficies tridimensionales\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "print (\"Prueba que si guarda xd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regresión Lineal con 1 variable\n",
    "\n",
    "La primera parte de la tarea trata de realizar regresión lineal utilizando únicamente 1 variable. \n",
    "\n",
    "El archivo que contiene los datos que se utilizará para la regresión lineal se llama `datos.txt`. El siguiente código se utiliza para leer los archivos y cargarlos como vectores numpy `x` (variable independiente) y `y` (variable que se desea predecir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de instancias:  97\n",
      "Tamaño de X:  (1, 97)\n",
      "Tamaño de y:  (1, 97)\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('datos.txt', delimiter=\",\")\n",
    "X = data[:,0]\n",
    "y = data[:,1]\n",
    "# Número de instancias (tamaño de la muestra)\n",
    "n = y.size\n",
    "# Convertir a un \"vector fila\" donde cada columna indica una instancia\n",
    "y = y.reshape(1,n)\n",
    "X = X.reshape(1,n)\n",
    "\n",
    "print(\"Número de instancias: \", n)\n",
    "print(\"Tamaño de X: \", X.shape)\n",
    "print(\"Tamaño de y: \", y.shape)\n",
    "# Si se desea inspeccionar los datos, descomentar la siguiente línea\n",
    "# print(\"X=\", X); print(\"y=\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que solo se tiene una variable independiente y una dependiente, los datos se pueden visualizar utilizando la función `scatter` de `matplotlib` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.colors' has no attribute 'to_rgba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;34m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# determine if a background is needed for legibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# the background is transparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_is_transparent\u001b[0;34m(color)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;34m\"\"\"Determine transparency from alpha.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.colors' has no attribute 'to_rgba'"
     ]
    }
   ],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Datos para regresión lineal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Función de Hipótesis\n",
    "\n",
    "Completar la siguiente función, llamada `h`, que representa la función de hipótesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X, w, w0):\n",
    "    \"\"\"\n",
    "    Calcula la función de hipótesis para la regresión lineal.\n",
    "    \n",
    "    Argumentos\n",
    "    ----------\n",
    "        X - Matriz de tamaño (d,n) que contiene cada una de las n instancias como columnas. Se \n",
    "            considera que cada instancia tiene d atributos.\n",
    "        w - Vector columna [w1;w2;w3;...] de tamaño (d,1) que contiene los parámetros wi (i=1,...,d)\n",
    "            del modelo\n",
    "        w0 - \"Bias\" del modelo (escalar)\n",
    "        \n",
    "    Retorna\n",
    "    -------\n",
    "        h(X) - Vector fila de tamaño (d,1) que contiene la predicción de cada una de las instancias.\n",
    "        \n",
    "    \"\"\"\n",
    "    # ====================== Completar aquí =====================\n",
    "    \n",
    "\n",
    "    \n",
    "    return 0   # Una vez completado, borrar este 0 y retornar el valor de h(X)\n",
    "    # ===========================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código se puede ejecutar solamente para validar la correcta implementación de la función de hipótesis. El valor esperado de la salida es aproximadamente 512.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.5]])\n",
    "w0 = 1.2\n",
    "print(\"Con w0=1.2 y w1=0.5, la suma de los valores predichos por h es: %.2f\" % np.sum(h(X, w, w0)) )\n",
    "print(\"Valor esperado: 512.15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Función de Costo\n",
    "\n",
    "Completar la siguiente función llamada `calcular_costo`, la cual debe implementar la función de costo $J(\\mathbf w)$ para la regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_costo(X, y, w, w0):\n",
    "    \"\"\"\n",
    "    Calcula el costo para la regresión lineal.\n",
    "    \n",
    "    Argumentos\n",
    "    ----------\n",
    "         X - Matriz de tamaño (d,n) que contiene cada una de las n instancias como columnas. Se considera\n",
    "            que cada instancia tiene d atributos.\n",
    "         y - Vector fila de tamaño (1,n) que contiene los valores que se desea predecir.\n",
    "         w - Vector columna de tamaño (d,1) que contiene los parámetros wi (i=1,...,d) del modelo\n",
    "        w0 - Parámetro que representa el \"bias\" del modelo (escalar)\n",
    "    \n",
    "    Retorna\n",
    "    -------\n",
    "        J - Costo de la regresión lineal\n",
    "        \n",
    "    \"\"\"\n",
    "   \n",
    "    # Número de instancias\n",
    "    n = y.size\n",
    "    # Valor de costo inicial (se puede borrar esto luego de implementar el costo real)\n",
    "    J = 0\n",
    "    # ====================== Completar aquí =====================\n",
    "\n",
    "    \n",
    "    # ===========================================================\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar la implementación de la función de costo, se puede ejecutar el siguiente código. Las segundas líneas muestran los valores esperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba 1 de la función de costo\n",
    "J = calcular_costo(X, y, np.array([[0.0]]), 0.0)\n",
    "print('Con w1=0, w0=0, el costo calculado es: %.2f' % J)\n",
    "print('Valor esperado: 32.07\\n')\n",
    "\n",
    "# Prueba 2 de la función de costo\n",
    "J = calcular_costo(X, y, np.array([[2]]), -1)\n",
    "print('Con w1=2, w0=-1, el costo calculado es: %.2f' % J)\n",
    "print('Valor esperado: 54.24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Descenso del Gradiente\n",
    "\n",
    "Completar la siguiente función llamada `gradient_descent`, la cual debe implementar la optimización de los parámetros del modelo de regresión lineal utilizando el método del descenso del gradiente. \n",
    "\n",
    "Notar que se requiere implementar un bucle que itere el número de veces dado por `num_iters`. La variable `J_history` debe almacenar el costo obtenido en cada iteración, con el único fin de más adelante poder observar la evolución de la función de costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, w0, alfa, num_iters=1500):\n",
    "    \"\"\"\n",
    "    Calcula los parámetros que minimizan la función de costo usando descenso de gradiente.\n",
    "    \n",
    "    Argumentos\n",
    "    ----------\n",
    "           X - Matriz de tamaño (d,n) que contiene cada una de las n instancias como columnas. Se considera\n",
    "               que cada instancia tiene d atributos.\n",
    "           y - Vector fila de tamaño (1,n) que contiene los valores que se desea predecir.\n",
    "           w - Vector columna de tamaño (d,1) que contiene los parámetros inniciales wi (i=1,...,d) del modelo\n",
    "          w0 - Parámetro que representa el \"bias\" inicial del modelo (escalar)\n",
    "        alfa - Factor de aprendizaje\n",
    "        num_iters - Número de iteraciones que realizará el algoritmo\n",
    "    \n",
    "    Retorna\n",
    "    -------\n",
    "        w - Vector columna que contiene los parámetros optimizados del modelo\n",
    "        w0 - Parámetro (escalar) que representa el bias optimizado\n",
    "        J_hist - Vector que almacena el valor de la función de costo en cada iteración\n",
    "        \n",
    "    \"\"\"\n",
    "    n = y.size\n",
    "    J_history = np.zeros(num_iters)\n",
    "    \n",
    "    # ====================== Completar aquí =====================\n",
    "\n",
    "    \n",
    "        \n",
    "    # ===========================================================\n",
    "    return(w, w0, J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar el correcto funcionamiento de la optimización utilizando el algoritmo del descenso del gradiente se puede utilizar directamente la siguiente sección de aplicación al problema inicial. Allí se muestra los parámetros que se esperaría obtener."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Entrenamiento\n",
    "\n",
    "En esta parte se realiza el entrenamiento del sistema para determinar los parámetros $w_0$, $w_1$ que se ajustan mejor a los datos iniciales. \n",
    "\n",
    "Primero se calculará los parámetros $w_0$, $w_1$ usando el descenso del gradiente, se graficará la función de costo en función del número de iteraciones, y se realizará el gráfico de la recta que se ajusta mejor a los datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de los parámetros\n",
    "w = np.zeros((1,1))   # Vector\n",
    "w0 = 0                # Escalar\n",
    "\n",
    "# Parámetros del descenso de gradiente\n",
    "iteraciones = 1500\n",
    "alfa = 0.01\n",
    "\n",
    "# Aplicación del descenso del gradiente\n",
    "w, w0, J_history = gradient_descent(X ,y, w, w0, alfa, iteraciones)\n",
    "print('Parámetros obtenidos con descenso del gradiente: {:.4f}, {:.4f}'.format(w0, w[0,0]))\n",
    "print('Parámetros esperados: w0=-3.6303, w1=1.1664')\n",
    "\n",
    "plt.plot(J_history)\n",
    "plt.ylabel('Costo  $J(w_0,w_1)$')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.title('Evolución de la función de costo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de la regresión con los puntos de entrenamiento\n",
    "plt.scatter(X,y)\n",
    "\n",
    "y_est = h(X, w, w0)\n",
    "plt.plot(X.T, y_est.T, 'r-')\n",
    "plt.legend(['Training data', 'Linear regression']);\n",
    "\n",
    "plt.xlabel(\"X\");\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes líneas grafican la función de costo obtenida en un rango de parámetros determinado, así como las curvas de nivel equivalente. Obsérvese que es más fácil visualizar qué sucede realmente en las curvas de nivel que en el gráfico tridimensional. Las curvas de nivel también muestran con un punto rojo el valor de w0, w1 obtenido con el método del descenso de gradiente para la regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calores para calcular la función de costo\n",
    "ww0 = np.linspace(-10, 10, 100)\n",
    "ww1 = np.linspace(-1, 4, 100)\n",
    "# Inicializar JJ a una matriz de ceros\n",
    "JJ = np.zeros((ww0.shape[0], ww1.shape[0]))\n",
    "# Calcular los valores de la función de costo JJ\n",
    "for i, w0_tmp in enumerate(ww0):\n",
    "    for j, w1_tmp in enumerate(ww1):\n",
    "        JJ[j, i] = calcular_costo(X, y, w1_tmp, w0_tmp)\n",
    "\n",
    "# Gráfico 3D\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(ww1, ww0, JJ, alpha=0.6, cmap='jet')\n",
    "plt.xlabel('$w_1$', fontsize=14)\n",
    "plt.ylabel('$w_0$', fontsize=14)\n",
    "plt.title('Costo J')\n",
    "\n",
    "# Curvas de nivel\n",
    "ax = plt.subplot(122)\n",
    "plt.contour(ww0, ww1, JJ, linewidths=2, cmap='jet', levels=np.logspace(-2, 3, 20))\n",
    "plt.xlabel('$w_0$'); plt.ylabel('$w_1$')\n",
    "plt.title('Curvas de nivel')\n",
    "\n",
    "# Mínimo encontrado con descenso del gradiente, en las curvas de nivel (punto rojo)\n",
    "plt.plot(w0, w[0,0], 'ro', ms=10, lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Predicción\n",
    "\n",
    "Una vez entrenado el modelo, se desea predecir los valores de salida para los valores $x_a=35$, $x_b=40$, $x_c=45$. Implementar las líneas de código necesarias para realizar esta predicción con el modelo entrenado en la sección 1.4. Se debe utilizar la función `print` para mostrar los valores de salida de la predicción.\n",
    "\n",
    "Se puede obtener la predicción para ambos valores por separado, pero se recomienda crear un vector (bidimensional) conteniendo los valores de entrada, para realizar la predicción de una sola vez a todos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Completar aquí =====================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Comparación con Scikit-learn\n",
    "\n",
    "Scikit-learn es una biblioteca que contiene diversos tipos de algoritmos de aprendizaje automático para Python. En esta sección (opcional) se desea comparar el resultado obtenido con la implementación anterior, con el resultado que brinda Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X.T, y.T)\n",
    "sk_w0 = regr.intercept_\n",
    "sk_w1 = regr.coef_\n",
    "plt.plot(X.T, sk_w0+sk_w1*X.T, 'b--', label='Linear regression (Scikit-learn GLM)')\n",
    "plt.plot(X.T, y_est.T, 'r--');\n",
    "\n",
    "regr.predict(np.array([[35], [40], [45]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regresión Lineal Multivariable\n",
    "\n",
    "Para esta parte, el archivo `datos2.txt` contiene los datos de entrenamiento necesarios. Las primeras dos columnas contienen los dos atributos de entrada. La tercera columna contiene el valor deseado de la salida. Es decir, en este caso se tendrá $\\mathbf x \\in \\mathbb R^2$, $y \\in \\mathbb R$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(\"datos2.txt\", delimiter=',')\n",
    "X = data[:, 0:2]   # Primeras dos columnas\n",
    "X = X.T            # Para tener [x(1) x(2) ... x(n)]\n",
    "y = data[:, 2]     # Tercera columna\n",
    "y = y[None]        # Convertir y a vector fila\n",
    "n = y.size\n",
    "\n",
    "# Tamaños\n",
    "print(\"Número de instancias: \", n)\n",
    "print(\"Tamaño de X: \", X.shape)\n",
    "print(\"Tamaño de y: \", y.shape)\n",
    "\n",
    "# Mostrar los 5 primeros datos\n",
    "print('\\n{:>8s}{:>11s}{:>6s}'.format('X[0,:]', 'X[1,:]', 'y'))\n",
    "print('-'*28)\n",
    "for i in range(5):\n",
    "    print('{:8.0f}{:8.0f}{:12.0f}'.format(X[0,i], X[1,i], y[0,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tener una visión más amplia de los datos, se puede realizar un histograma de los dos atributos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1); plt.hist(X[0,:])\n",
    "plt.title('Histograma de $x_1$')\n",
    "\n",
    "plt.subplot(1,2,2); plt.hist(X[1,:])\n",
    "plt.title('Histograma de $x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Modificación de la Escala de los Atributos\n",
    "\n",
    "Como se observa en los datos mismos, y en los histogramas, el rango de ambos atributos es muy diferente, por lo que es necesario modificar su escala; es decir, normalizarlos. Completar la función `normalizar` que aplica una normalización de la media a cada uno de los atributos. \n",
    "\n",
    "Dentro de la función se puede usar las funciones `np.mean` y `np.std` que calculan la media y la desviación estándar, respectivamente, pero se debe tener cuidado con los datos a los cuales se aplica (lo cual se puede escoger con la opción `axis`). También se sugiere utilizar el parámetro `keepdims=True` en estas dos funciones para mantener el tamaño del vector (de lo contrario sería convertido a un arreglo unidimensional). Para mayor detalle, consultar la documentación de estas funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  normalizar(X):\n",
    "    \"\"\"\n",
    "    Normaliza cada atributo de X usando la media y la desviación estándar\n",
    "    \n",
    "    Argumentos\n",
    "    ----------\n",
    "        X - Matriz (d,n) que contiene todos cada una de las n instancias como columnas. Se considera\n",
    "            que cada instancia tiene d atributos.\n",
    "    \n",
    "    Returna\n",
    "    -------\n",
    "        Xnorm - Matriz (d,n) que contiene cada atributo normalizado.\n",
    "          mu  - Vector de tamaño (d,1) que contiene las medias de cada atributo \n",
    "        sigma - Vector de tamaño (d,1) que contiene las desviaciones estándar de cada atributo\n",
    "        \n",
    "    \"\"\"\n",
    "    # Copia la matriz de entrada X\n",
    "    Xnorm = X.copy()\n",
    "\n",
    "    # ======================= Completar aquí =========================\n",
    "\n",
    "    \n",
    "    \n",
    "    # ================================================================\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar el correcto funcionamiento, se puede ejecutar las siguientes líneas de código. El resultado esperado de las medias es 2000.68 y 3.17, y el resultado de las desviaciones estándar es 786.2 y 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar los datos\n",
    "Xnorm, mu, sigma = normalizar(X)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print('Media obtenida:', np.round(mu.T,2))\n",
    "print('Media esperada: [2000.68, 3.17]')\n",
    "print('Desviación estándar obtenida:', np.round(sigma.T,2))\n",
    "print('Desviación estándar esperada: [786.20, 0.75]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, se puede verificar el funcionamiento de la función `normalizar` realizando nuevamente un histograma de cada uno de los atributos de X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Xnorm[0,:], label=\"$x_1$\")\n",
    "plt.hist(Xnorm[1,:], label=\"$x_2$\")\n",
    "plt.title('Histograma de atributos normalizados')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Entrenamiento\n",
    "\n",
    "Con base en el entrenamiento realizado para el caso de una variable, realizar el entrenamiento de los parámetros para este caso donde hay dos atributos en los datos de entrada.\n",
    "\n",
    "Encontrar un valor adecuado para el factor de aprendizaje `alfa`, de tal modo que la función de costo converja antes de 50 iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inicialización de los parámetros\n",
    "w = np.zeros((2,1))   # Vector que contiene w1, w2\n",
    "w0 = 0                # Escalar\n",
    "\n",
    "# =============  Completar aquí ========================================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "\n",
    "print('Parámetros obtenidos con descenso del gradiente:')\n",
    "print('w0={:.4f}, w1={:.4f}, w2={:.4f}'.format(w0, w[0,0], w[1,0]))\n",
    "\n",
    "plt.plot(J_history)\n",
    "plt.ylabel('Costo  $J(w_0,w_1)$')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.title('Evolución de la función de costo');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Predicción\n",
    "\n",
    "Luego de tener el sistema entrenado, se desea predecir el valor que se obtendría si el dato de entrada fuese `(1650, 3)`. Se debe tener en cuenta que primero se debe normalizar el valor (con la media y desviación estándar antes calculadas) antes de aplicarlo a la función de predicción. El valor de predicción debe ser aproximadamente 2930.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Completar aquí ===============\n",
    "\n",
    "\n",
    "\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ecuación Normal\n",
    "\n",
    "En esta parte se desea realizar la predicción de la parte 2 pero utilizando la ecuación normal. Debido a que en este caso no se tiene muchos atributos ni instancias, esto es viable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Determinación de los Parámetros\n",
    "\n",
    "Utilizando la ecuación normal, determinar los parámetros w0, w1, w2. Para esta implementación se requiere que el vector X contenga cada instancia como una fila (a diferencia de la implementación anterior donde cada instancia era una columa). Por este motivo, se utilizará la transpuesta, y la matriz de entrada que se utilizará es `Xn`. Lo mismo sucede con el vector de salidas: se utilizará `yn`.\n",
    "\n",
    "Notar que en este caso sí se requiere utilizar un atributo $x_0=1$ para cada instancia (esto ya se realizó en el código siguiente usando `np.ones` y `np.hstack`).\n",
    "\n",
    "Cuando se utiliza la ecuación normal no es necesario ningún tipo de modificación de la escala de los atributos, como se realizó anteriormente, debido a que se obtiene una respuesta en forma cerrada y no iterativa. Así, en esta parte no habrá normalización de atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de instancias\n",
    "n = X.shape[1]\n",
    "# Se transpone tanto X como y para tener la forma utilizada en la ecuación normal\n",
    "Xn = X.T\n",
    "yn = y.T\n",
    "# Se añade un \"1\" a cada atributo (como atributo x0)\n",
    "Xn = np.hstack([np.ones([n,1]), Xn])\n",
    "\n",
    "# ================== Completar aquí ==============================\n",
    "wn = 0   # Borrar esto una vez completado\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "print(\"Valores de w obtenidos:\\n\", wn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predicción\n",
    "\n",
    "Utilizando los parámetros obtenidos con la ecuación normal, predecir el valor de salida que se obtendría si el dato de entrada fuese `(1650, 3)`. El resultado debe ser similar al obtenido usando descenso del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Completar aquí ==============================\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
